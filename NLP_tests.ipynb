{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\romoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\romoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\romoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "import torch\n",
    "from datasets import Dataset , Sequence , Value , Features , ClassLabel , DatasetDict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Язык текста: ru\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Роднее всех родных Попала я в ГКБ №8 еще в декабре 2002 г. Ехать в больницу очень боялась, но делать нечего, рожать надо. О ГКБ № 8 слышала много хорошего. Когда приехала в приемное отделение, мои колени тряслись от страха, но меня там встретили очень вежливо. Осмотр врача был аккуратным и осторожным, во всем чувствовалась забота и внимание. После того, как меня определили в палату, моим врачом стала Попова Любовь Васильевна. Я благодарна судьбе за то, что она свела меня с таким прекрасным человеком. Она огромнейшей души человек и супер-врач. Внимательна, тактична, отвечала на все, порой даже глупые вопросы. Всегда может подбодрить, улыбнуться. Расскажет зачем и какие препараты нам назначают, а также зачем и какие анализы мы сдаем. С ней я себя чувствовала как за каменной стеной. К тому же у нас собралась веселая палата. На этаже есть телевизор, работае с 17 до 22 часов. Вообщем-то этого вполне хватало, т. к. у каждого были свои процедуры, капельницы, уколы и т. д. По поводу уколов и капельниц хочется сказать отдельное спасибо Анжеле. Рука у нее легкая, да и сама она, что называется, своя девчонка. Без всяких выпендрежей и заморочек. Также хочется сказать спасибо и сестре - хозяйке (к сожалению, не помню ее имя). Сперва она показалась нам очень шумной, но потом мы не представляли себе и дня без нее. Она всегда была с нами внимательна. Всегда вовремя меняла постельное белье, пеленки, полотенца. Всегда следила за тем, чтобы были чистыми холодильники. Должна сказать, что работа в отделении патологии налажена отлично. Никто не суетится, не бегает, не кричит. Все четко знают свои обязанности и оперативно выполняют работу. Все это благодаря хорошим организаторским способностям заведующей отделением, Аландровой Татьяны Вычеславовны. За то время, что я провела в больнице на обследовании, я так всех полюбила, что если бы не Новый год, то уходить не хотелось. А мой врач - Любовь Васильевна, стала для меня роднее всех родных.\"\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        if language == 'ru':\n",
    "            lang = \"ru\"\n",
    "        elif language == 'en':\n",
    "            lang = \"en\"\n",
    "        else:\n",
    "            lang = \"Неверный язык\"\n",
    "        return lang\n",
    "    except Exception as e:\n",
    "        return f'Ошибка при определении языка: {e}'\n",
    "    return\n",
    "\n",
    "detected_language = detect_language(sample_text)\n",
    "print(f'Язык текста: {detected_language}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detected_language == 'ru':\n",
    "    df_emoji = pd.read_csv(\"E:/project_1_for_NLP/emoji_dataset_russian.csv\")\n",
    "    df_slang = pd.read_csv(\"E:/project_1_for_NLP/russian_slang.csv\")\n",
    "    emoji_dict = dict(zip(df_emoji['emoji'], df_emoji['description_ru']))\n",
    "    slang_dict = dict(zip(df_slang['acronym'], df_slang['expansion']))\n",
    "if detected_language == 'en':\n",
    "    df_slang = pd.read_csv('slang/slang/slang.csv')\n",
    "    df_emoji = pd.read_csv('emoji/emoji_df.csv')\n",
    "    emoji_dict = dict(zip(df_emoji['emoji'], df_emoji['name']))\n",
    "    slang_dict = dict(zip(df_slang['acronym'], df_slang['expansion']))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def replace_emoji_with_words(text):\n",
    "    for emj, desc in emoji_dict.items():\n",
    "        text = text.replace(emj, f\" {desc} \")\n",
    "    return text\n",
    "\n",
    "def replace_slang_with_full_form(text):\n",
    "    words = text.split()\n",
    "    new_words = [slang_dict.get(word.lower(), word) for word in words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocessText(text):\n",
    "    text = convert_to_lower(text)\n",
    "    text = replace_emoji_with_words(text)\n",
    "    text = replace_slang_with_full_form(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_extra_white_spaces(text)\n",
    "    return text\n",
    "\n",
    "def preprocessBatch(batch):\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in batch[\"text\"]:\n",
    "        text = convert_to_lower(text)\n",
    "        text = replace_emoji_with_words(text)\n",
    "        text = replace_slang_with_full_form(text)\n",
    "        text = remove_punctuation(text)\n",
    "        text = remove_extra_white_spaces(text)\n",
    "        processed_texts.append(text)\n",
    "    batch[\"text\"] = processed_texts\n",
    "    return batch\n",
    "\n",
    "if detected_language == 'ru':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('seara/rubert-tiny2-russian-sentiment')\n",
    "    max_len = 512\n",
    "if detected_language == 'en':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    max_len = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForClassification(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                labels=None, **kwargs):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "\n",
    "        sequence_output = self.dropout(outputs[1])\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romoc\\AppData\\Local\\Temp\\ipykernel_19768\\2457069868.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bert_model.load_state_dict(torch.load('bert_model_rus.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if detected_language == 'ru':\n",
    "    id2label = {\n",
    "    0:  'Neutral',\n",
    "    1: 'Positive',\n",
    "    2: 'Negative'\n",
    "    }\n",
    "\n",
    "    label2id = { v:k for (k,v) in id2label.items()}\n",
    "    class_names = [ 'Neutral', 'Positive', 'Negative']\n",
    "    model_name = 'seara/rubert-tiny2-russian-sentiment'\n",
    "    bert_config = AutoConfig.from_pretrained(model_name, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "    bert_model = (BertForClassification.from_pretrained(model_name, config=bert_config).to(device))\n",
    "    bert_model.load_state_dict(torch.load('bert_model_rus.pth'))\n",
    "if detected_language == 'en':\n",
    "    id2label = {\n",
    "    0: 'Negative',\n",
    "    1: 'Neutral',\n",
    "    2: 'Positive'\n",
    "    }\n",
    "    label2id = { v:k for (k,v) in id2label.items()}\n",
    "    class_names = [ 'Negative','Neutral', 'Positive']\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=3, id2label=id2label, label2id=label2id)\n",
    "    bert_model = (BertForClassification.from_pretrained('bert-base-uncased', config=config).to(device))\n",
    "    bert_model.load_state_dict(torch.load('bert_model_eng.pth'))\n",
    "    \n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(sample_text):\n",
    "    sample_text = preprocessText(sample_text)\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        sample_text,\n",
    "        max_length=max_len,\n",
    "        add_special_tokens=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    output = bert_model(input_ids, attention_mask)\n",
    "    _,preds = torch.max(output.logits, dim=1)\n",
    "    pd.DataFrame({\n",
    "        \"Text\": sample_text,\n",
    "        \"Sentiment\": class_names[preds]\n",
    "    },index=[0]).T\n",
    "    return class_names[preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romoc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "result = eval_model(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тональность текста \"Роднее всех родных Попала я в ГКБ №8 еще в декабре 2002 г. Ехать в больницу очень боялась, но делать нечего, рожать надо. О ГКБ № 8 слышала много хорошего. Когда приехала в приемное отделение, мои колени тряслись от страха, но меня там встретили очень вежливо. Осмотр врача был аккуратным и осторожным, во всем чувствовалась забота и внимание. После того, как меня определили в палату, моим врачом стала Попова Любовь Васильевна. Я благодарна судьбе за то, что она свела меня с таким прекрасным человеком. Она огромнейшей души человек и супер-врач. Внимательна, тактична, отвечала на все, порой даже глупые вопросы. Всегда может подбодрить, улыбнуться. Расскажет зачем и какие препараты нам назначают, а также зачем и какие анализы мы сдаем. С ней я себя чувствовала как за каменной стеной. К тому же у нас собралась веселая палата. На этаже есть телевизор, работае с 17 до 22 часов. Вообщем-то этого вполне хватало, т. к. у каждого были свои процедуры, капельницы, уколы и т. д. По поводу уколов и капельниц хочется сказать отдельное спасибо Анжеле. Рука у нее легкая, да и сама она, что называется, своя девчонка. Без всяких выпендрежей и заморочек. Также хочется сказать спасибо и сестре - хозяйке (к сожалению, не помню ее имя). Сперва она показалась нам очень шумной, но потом мы не представляли себе и дня без нее. Она всегда была с нами внимательна. Всегда вовремя меняла постельное белье, пеленки, полотенца. Всегда следила за тем, чтобы были чистыми холодильники. Должна сказать, что работа в отделении патологии налажена отлично. Никто не суетится, не бегает, не кричит. Все четко знают свои обязанности и оперативно выполняют работу. Все это благодаря хорошим организаторским способностям заведующей отделением, Аландровой Татьяны Вычеславовны. За то время, что я провела в больнице на обследовании, я так всех полюбила, что если бы не Новый год, то уходить не хотелось. А мой врач - Любовь Васильевна, стала для меня роднее всех родных.\" : Positive\n"
     ]
    }
   ],
   "source": [
    "print(f'Тональность текста \"{sample_text}\" : {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
